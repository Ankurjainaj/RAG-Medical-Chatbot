# -*- coding: utf-8 -*-
"""assignment_1_Q1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1umZXVMveOuZiDDFgA-RFfl9S8Zb-qabm

# LINEAR REGRESSION

# SECTION 1.1 DATA VISUALISATION AND PLOTS

1 A) DATA VISUALISATION
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

data = pd.read_csv('California-Housing-Dataset.csv')
df = pd.DataFrame(data)
df = df.dropna(axis=0)
df.head()

"""TRAIN TEST SPLIT"""

''' I have made a custom train test split function to split the dataframe into test and train '''
''' frac=1 means whole data will be taken, split ration determines how much data will be test and train'''

def train_test_split(df, split_ratio=0.8):   #split ratio of data
  if split_ratio<=0 or split_ratio>=1:
    print('Invalid split ratio')
    return
  df = df.sample(frac=1, random_state=42).reset_index(drop=True)  #shuffling data first

  split_index = int(split_ratio * len(df))
  train_data = df[:split_index]
  test_data = df[split_index:]
  return train_data, test_data

''' Here the label data is being dropped from X_train and X_test '''
''' Y_test and Y_train are made up of only one column the median house value '''

train_data, test_data = train_test_split(df, 0.8)  #taken 0.8 by default

X_train = train_data.drop('median_house_value', axis=1)
Y_train = train_data[['median_house_value']]

X_test = test_data.drop('median_house_value', axis=1)
Y_test = test_data[['median_house_value']]

"""1 B) SCATTER PLOT OF EACH ATTRIBUTE V/S LABEL"""

''' plotting of each label v/s attribute  '''
cols = X_train.shape[1]

nrows = (cols + 1) // 2

fig, axes = plt.subplots(nrows=nrows, ncols=2, figsize=(12, 5 * nrows))

axes = np.array(axes).flatten()


for i in range(cols):
  feature = X_train.columns[i]

  ax = axes[i]
  ax.scatter(X_train.iloc[ : , i], Y_train, color='blue', label='Train')
  ax.scatter(X_test.iloc[ : , i], Y_test, color='red', label='Test')
  ax.set_xlabel(feature)
  ax.set_ylabel(Y_train.iloc[0])
  ax.set_title(f'{ feature } vs Median-House-Value')
  ax.legend()


for j in range(cols, len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()
plt.show()

''' Here since the last column ocean proximity is not numerical i have dropped it as it was getting difficult to handle '''

X_train.drop('ocean_proximity', axis=1, inplace=True)
X_test.drop('ocean_proximity', axis=1, inplace=True)  # had to drop ocean proximity as it was giving error in gradient descent

X_train = np.asarray(X_train, dtype=float)
X_test = np.asarray(X_test, dtype=float)
Y_train = np.asarray(Y_train, dtype=float)
Y_test = np.asarray(Y_test, dtype=float)

''' The standardise function is made to normalise the values of X_train and X_test '''
''' The given values are very large and their scaling is very far apart '''

def standardise(X_train, X_test):
  # X_train = np.asarray(X_train, dtype=float)   # to change from dataframe to np ndarray
  # X_test = np.asarray(X_test, dtype=float)

  X_train_std = X_train.copy()
  X_test_std = X_test.copy()
  means = np.mean(X_train[:,1:], axis=0)
  std = np.std(X_train[:,1:], axis=0)
  std[std==0] = 1

  X_train_std[:, 1:] = (X_train[:, 1:]-means)/std
  X_test_std[:, 1:] = (X_test[:, 1:]-means)/std

  return X_train_std, X_test_std

''' This function is made to standardise the label value for prediction and error later on '''

def standardise_label(y):
    y = np.asarray(y, dtype=float)
    mean_y = np.mean(y)
    std_y = np.std(y)
    y_std = (y - mean_y)/std_y
    return mean_y, std_y, y_std

"""1 C) PLOYNOMIAL CREATION FOR LINEAR REGRESSION + GRADIENT DESCENT"""

''' This function creates the feature matrix  '''
''' cols is the bias column added to the feature matrix '''

def create_multi_polynomial(X, degree):
  m, n = X.shape
  cols = [np.ones((m,1))]   # add one bias column consisting of all ones

  for j in range(n):
    for d in range(1, degree+1):  # starting from 1 as 0th is added already
      cols.append((X[ : , j] ** d).reshape(-1, 1))
  return np.hstack(cols)

''' This is the gradient descent function which calculates and converges the weight matrix '''

def gradient_descent_fun(X, Y, alpha=0.05, epochs=1000):

  m, n = X.shape
  costs = []
  w = np.zeros((n,1))   # for each feature weight = 0 by default

  for _ in range(epochs):
    y_pred = X.dot(w)
    error  = y_pred - Y
    cost = 1/(2*m)*np.sum(error**2)
    costs.append(cost)
    grad = X.T.dot(error)
    w = w-(1/m)*alpha*grad
  return w, costs

''' Ridge regularisation function is applied on existing gradient descent '''
''' This is done to reduce overfitting '''

def ridge_regularisation(X, Y, ilambda, alpha=0.05, epochs=1000):

  m, n = X.shape
  costs = []
  w = np.zeros((n,1))

  for _ in range(epochs):
    y_pred = X.dot(w)
    error  = y_pred - Y
    cost = (1/(2*m))*np.sum(error**2) + (ilambda)*np.sum(w[1:]**2)
    costs.append(cost)

    grad = (1/m)*X.T.dot(error)
    reg_adjustment = np.vstack([[0],2*w[1:]]) #remove the bias term from regression in ridge regression
    grad = (ilambda)*reg_adjustment + grad    # removed "/m" from ilambda/m as it was rendering lambda value useless due to very small lambda values given
    w = w-alpha*grad
  return w, costs

''' Lasso regression helps in feature selection '''

def lasso_regularisation(X, Y, ilambda, alpha=0.05, epochs=1000):

  m, n = X.shape
  costs = []
  w = np.zeros((n,1))

  for _ in range(epochs):
    y_pred = X.dot(w)
    error  = y_pred - Y
    cost = 1/(2*m)*np.sum(error**2) + (ilambda)*np.sum(np.abs(w[1:]))  # here we divided by m only because during differentiation no 2 must be cancelled out
    costs.append(cost)

    grad = (1/m)*X.T.dot(error)

    # reg_adjustment = np.sign(w)
    # reg_adjustment[0] = 0               earlier i was doing it without feature selection
    # grad = ilambda*reg_adjustment + grad
    w = w-alpha*grad

    threshold_term = alpha * (ilambda)    # earlier i was using ilambda/m for threshold, but it was making lambda useless so i removed m, mainly due to lambda values very small
    for j in range(1, n):   # applied soft thresholding for feature selection
      if w[j] > threshold_term:
        w[j] = w[j] - threshold_term
      elif w[j] < -(threshold_term):
        w[j] = w[j] + threshold_term
      else:
        w[j] = 0
  return w, costs

''' THis is the driver function for regularisation '''

def regularisation(X_train, X_test,Y_train, Y_test,Y_std, Y_mean,Y_train_std, degree, ilambda):
  X_train_Phi = create_multi_polynomial(X_train, degree)
  X_test_Phi = create_multi_polynomial(X_test, degree)
  X_train_Phi_std, X_test_Phi_std = standardise(X_train_Phi, X_test_Phi)

  ridge_train_error = []
  ridge_test_error = []
  lasso_train_error = []
  lasso_test_error = []

  for lam in ilambda:
    W_ridge, _ = ridge_regularisation(X_train_Phi_std, Y_train_std, lam)

    y_train_predict_ridge = (X_train_Phi_std.dot(W_ridge)) * Y_std + Y_mean
    y_test_predict_ridge = (X_test_Phi_std.dot(W_ridge)) * Y_std + Y_mean

    mse_ridge_train = np.mean((Y_train - y_train_predict_ridge)**2)
    ridge_train_error.append(mse_ridge_train)

    mse_ridge_test = np.mean((Y_test - y_test_predict_ridge)**2)
    ridge_test_error.append(mse_ridge_test)


    W_lasso, _ = lasso_regularisation(X_train_Phi_std, Y_train_std, lam)

    y_train_predict_lasso = (X_train_Phi_std.dot(W_lasso)) * Y_std + Y_mean
    y_test_predict_lasso = (X_test_Phi_std.dot(W_lasso)) * Y_std + Y_mean

    mse_lasso_train = np.mean((Y_train - y_train_predict_lasso)**2)
    lasso_train_error.append(mse_lasso_train)

    mse_lasso_test = np.mean((Y_test - y_test_predict_lasso)**2)
    lasso_test_error.append(mse_lasso_test)

  return ridge_train_error, ridge_test_error, lasso_train_error, lasso_test_error

def prediction(X, w):
  return X.dot(w)

"""# SECTION 1.2 DEGREE SELECTION FOR MINIMUM AND MAXIMUM MSE

2 A) PLOT OF GRAPHS VS DEGREES
"""

''' This block prints scatter plot and fits a line of regression on the scatter plot '''

degree = list(range(1, 10))
train_error = []
test_error = []
for d in degree:

  X_train_Phi = create_multi_polynomial(X_train, d)
  X_test_Phi = create_multi_polynomial(X_test, d)
  X_train_Phi_std, X_test_Phi_std = standardise(X_train_Phi, X_test_Phi)

  Y_mean, Y_std, Y_train_std = standardise_label(Y_train)# standardised training labels also to match the feature values, will reverse later on
  w, costs = gradient_descent_fun(X_train_Phi_std, Y_train_std , epochs=100)


  y_train_std_pred = prediction(X_train_Phi_std, w)
  y_train_pred = ((y_train_std_pred)*Y_std) + Y_mean    # converting from standardised y prediction to original
  mse = np.mean((Y_train-y_train_pred)**2)    # calculating the mse
  train_error.append(mse)


  y_test_std_pred = prediction(X_test_Phi_std, w)
  y_test_pred = ((y_test_std_pred)*Y_std) + Y_mean    #converting the standardised y prediction to original
  mse = np.mean((Y_test-y_test_pred)**2)    # calculate the mse
  test_error.append(mse)

  fig, axes = plt.subplots(1, 2, figsize=(12, 5))

  axes[0].plot(range(100), costs, marker='o')
  axes[0].set_xlabel('Epochs')
  axes[0].set_ylabel('Cost')
  axes[0].set_title('Cost vs epochs')


  axes[1].scatter(Y_test, y_test_pred, c='blue', marker='o')

  slope, intercept = np.polyfit(Y_test.ravel(), y_test_pred.ravel(), 1)
  x_line = np.linspace(Y_test.min(), Y_test.max(), 100)
  y_line = slope * x_line + intercept

  axes[1].plot(x_line, y_line, 'r-', label=f"Model fit (slope={slope:.2f})")
  axes[1].set_title(f"Degree {d} (SE={mse:.2f})")
  axes[1].set_xlabel("True Price")
  axes[1].set_ylabel("Predicted Price")
  axes[1].legend()

costs

test_error

"""2 B) MINIMUM AND MAXIMUM ERROR DEGREE VISUALISATION"""

''' This block plots a graph between mean square error and degree of polynomial '''

plt.plot(range(1,10), train_error, marker='s', label='Train')
plt.plot(range(1,10), test_error, marker='o', label='Test')
plt.xlabel('Degree')
plt.ylabel('MSE')
plt.title('MSE vs Degree')
plt.legend(['Train', 'Test'])
plt.grid(True)
plt.show()

"""We can see that the  test error is least for n=6 degree for testing but least for n=5 degree for training

# SECTION 1.3 REGULARISATION
"""

max_test_error_deg = degree[np.argmax(test_error)]
min_test_error_deg = degree[np.argmin(test_error)]

Y_mean, Y_std, Y_train_std = standardise_label(Y_train)
ilambda = [0, 0.25, 0.5, 0.75, 1]

"""3 A) LASSO REGULARISATION ( FUNCTIONS MADE ABOVE IN SECTION 1)"""

ridge_min_train_error, ridge_min_test_error, lasso_min_train_error, lasso_min_test_error = regularisation(X_train, X_test, Y_train, Y_test, Y_std, Y_mean,Y_train_std, min_test_error_deg,ilambda)
ridge_max_train_error, ridge_max_test_error, lasso_max_train_error, lasso_max_test_error = regularisation(X_train, X_test, Y_train, Y_test, Y_std, Y_mean,Y_train_std, max_test_error_deg,ilambda)

ridge_min_test_error

''' plotting graph for maximum error degree as shown in degree vs mse graph between train and test error '''
plt.plot(ilambda, ridge_max_train_error, 'o-', label=f'Ridge Train (deg {max_test_error_deg})')
plt.plot(ilambda, ridge_max_test_error, 'o-', label=f'Ridge Test (deg {max_test_error_deg})')
plt.plot(ilambda, lasso_max_train_error, 's--', label=f'Lasso Train (deg {max_test_error_deg})')
plt.plot(ilambda, lasso_max_test_error, 's--', label=f'Lasso Test (deg {max_test_error_deg})')

plt.xscale('log')  # λ is usually on log scale
plt.xlabel('Lambda (Regularisation Strength)')
plt.ylabel('Mean Squared Error')
plt.title(f'Training vs Test Error (Degree {max_test_error_deg})')
plt.legend()
plt.grid(True)
plt.show()

"""3 B) RIDGE REGULARISATION ( FUNCTIONS MADE ABOVE IN SECTION 1 )"""

''' plotting graph for minimum error degree as shown in degree vs mse graph between train and test error '''
plt.plot(ilambda, ridge_min_train_error, 'o-', label=f'Ridge Train (deg {min_test_error_deg})')
plt.plot(ilambda, ridge_min_test_error, 'o-', label=f'Ridge Test (deg {min_test_error_deg})')
plt.plot(ilambda, lasso_min_train_error, 's--', label=f'Lasso Train (deg {min_test_error_deg})')
plt.plot(ilambda, lasso_min_test_error, 's--', label=f'Lasso Test (deg {min_test_error_deg})')

plt.xscale('log')  # λ usually on log scale
plt.xlabel('Lambda (Regularisation Strength)')
plt.ylabel('Mean Squared Error')
plt.title(f'Training vs Test Error (Degree {min_test_error_deg})')
plt.legend()
plt.grid(True)
plt.show()

"""As we can see that ridge regularisation is giving almost smooth curve and lower bias, so we can say that ridge regression is more efficient on the given dataset"""
